{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               student_id  chapter_number  \\\n",
      "44   041a771b-41cc-4b36-928d-ec321f918db5               2   \n",
      "45   041a771b-41cc-4b36-928d-ec321f918db5               2   \n",
      "46   041a771b-41cc-4b36-928d-ec321f918db5               2   \n",
      "47   041a771b-41cc-4b36-928d-ec321f918db5               2   \n",
      "84   04347fe5-e55a-4adc-86f2-a156ddfcfc13               2   \n",
      "85   04347fe5-e55a-4adc-86f2-a156ddfcfc13               2   \n",
      "86   04347fe5-e55a-4adc-86f2-a156ddfcfc13               2   \n",
      "87   04347fe5-e55a-4adc-86f2-a156ddfcfc13               2   \n",
      "128  045d0e87-585c-4126-a2c0-c744effb11b2               2   \n",
      "129  045d0e87-585c-4126-a2c0-c744effb11b2               2   \n",
      "\n",
      "                                                  book       EOC  \n",
      "44   College / Advanced Statistics and Data Science...  0.673469  \n",
      "45   College / Advanced Statistics and Data Science...  0.673469  \n",
      "46   College / Advanced Statistics and Data Science...  0.673469  \n",
      "47   College / Advanced Statistics and Data Science...  0.673469  \n",
      "84   College / Advanced Statistics and Data Science...  0.863636  \n",
      "85   College / Advanced Statistics and Data Science...  0.863636  \n",
      "86   College / Advanced Statistics and Data Science...  0.863636  \n",
      "87   College / Advanced Statistics and Data Science...  0.863636  \n",
      "128  College / Advanced Statistics and Data Science...  0.584746  \n",
      "129  College / Advanced Statistics and Data Science...  0.584746  \n"
     ]
    }
   ],
   "source": [
    "##checkpoint_eoc csv file\n",
    "##merge on book, chapter, student_id \n",
    "##filter per chapter, \n",
    "##eoc number of corrects, it's n_correct/ n_possible \n",
    "##filter on the response type and disregard the  na's\n",
    "\n",
    "\n",
    "#merge checkpoint_eoc.csv and checkpoint_pulse.cdv on stunt_id, book, chapter\n",
    "#get the EOC, n_attempy from checkpoint_eoc.csv\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "d1 = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Desktop\\\\datafest\\\\Random Sample of Data Files_03_04\\\\checkpoints_eoc.csv\")\n",
    "d2 = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Desktop\\\\datafest\\\\Random Sample of Data Files_03_04\\\\checkpoints_pulse.csv\")\n",
    "\n",
    "\n",
    "# Merge the two dataframes on student_id, book, chapter, including EOC and n_attempt\n",
    "d3 = pd.merge(d1, d2, on=['student_id', 'book', 'chapter_number'])\n",
    "\n",
    "# Sort per book and chapter\n",
    "d3 = d3.sort_values(['book', 'chapter_number'])\n",
    "\n",
    "#drop \n",
    "\n",
    "# Print 10 samples\n",
    "#d3.head(10)\n",
    "\n",
    "#drop class_id, n_possible, n_correct, response, construct, release \n",
    "d3 = d3.drop(['class_id_x', 'n_possible', 'n_correct', 'release','class_id_y','institution_id','response','n_attempt','response','construct'], axis=1)\n",
    "\n",
    "#just take the consttruct= Cost\n",
    "#d3 = d3[d3['construct'] == 'Cost']\n",
    "\n",
    "#drop nan values\n",
    "#d3 = d3.dropna(subset=['response'])\n",
    "print(d3.head(10))\n",
    "\n",
    "#create a new csv file at the same path\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                book  chapter_number  \\\n",
      "0  College / Advanced Statistics and Data Science...               2   \n",
      "1  College / Advanced Statistics and Data Science...               3   \n",
      "2  College / Advanced Statistics and Data Science...               4   \n",
      "3  College / Advanced Statistics and Data Science...               5   \n",
      "4  College / Advanced Statistics and Data Science...               6   \n",
      "5  College / Advanced Statistics and Data Science...               7   \n",
      "6  College / Advanced Statistics and Data Science...               8   \n",
      "7  College / Advanced Statistics and Data Science...               9   \n",
      "8  College / Advanced Statistics and Data Science...              10   \n",
      "9  College / Advanced Statistics and Data Science...              11   \n",
      "\n",
      "   difficulty  \n",
      "0    0.273057  \n",
      "1    0.306468  \n",
      "2    0.347523  \n",
      "3    0.334382  \n",
      "4    0.379361  \n",
      "5    0.370850  \n",
      "6    0.369938  \n",
      "7    0.405785  \n",
      "8    0.414719  \n",
      "9    0.492801  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming d3 is your DataFrame after any necessary merging and cleaning steps.\n",
    "\n",
    "# Calculate the mean EOC for each book and chapter\n",
    "mean_eoc_per_chapter = d3.groupby(['book', 'chapter_number'])['EOC'].mean().reset_index()\n",
    "\n",
    "# Define the difficulty metric as the inverse of the mean EOC score\n",
    "mean_eoc_per_chapter['difficulty'] = 1 - mean_eoc_per_chapter['EOC']\n",
    "\n",
    "# Merge this difficulty metric back into the original DataFrame on 'book' and 'chapter_number'\n",
    "d3_with_difficulty = d3.merge(mean_eoc_per_chapter[['book', 'chapter_number', 'difficulty']], \n",
    "                              on=['book', 'chapter_number'], \n",
    "                              how='left')\n",
    "\n",
    "# Create a new DataFrame with just the book, chapter number, and difficulty\n",
    "difficulty_only_df = mean_eoc_per_chapter[['book', 'chapter_number', 'difficulty']]\n",
    "\n",
    "# Now you have d3_with_difficulty which includes the original data plus a difficulty column,\n",
    "# and difficulty_only_df which contains only the book, chapter, and difficulty.\n",
    "print(difficulty_only_df.head(10))\n",
    "\n",
    "# Save the difficulty_only_df to a new CSV file on the samepath with this \"C:\\\\Users\\\\HP\\\\Desktop\\\\datafest\\\\Random Sample of Data Files_03_04\\\\checkpoints_pulse.csv\"\n",
    "difficulty_only_df.to_csv(\"C:\\\\Users\\\\HP\\\\Desktop\\\\datafest\\\\Random Sample of Data Files_03_04\\\\difficulty_only.csv\", index=False)\n",
    "\n",
    "#also  save the d3_with_difficulty to a new CSV file on the samepath with this \"C:\\\\Users\\\\HP\\\\Desktop\\\\datafest\\\\Random Sample of Data Files_03_04\\\\checkpoints_pulse.csv\"\n",
    "d3_with_difficulty.to_csv(\"C:\\\\Users\\\\HP\\\\Desktop\\\\datafest\\\\Random Sample of Data Files_03_04\\\\d3_with_difficulty.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Since we're working with a simulated dataset, we'll create conditions to remove outliers based on your description.\n",
    "# In your actual data, you would apply these conditions to your d3 DataFrame.\n",
    "\n",
    "# Let's first simulate the dataset with 'EOC', 'response', and 'n_attempt'.\n",
    "np.random.seed(0)  # Seed for reproducibility\n",
    "dummy_data = {\n",
    "    'EOC': np.random.uniform(0, 1, 1000),  # 1000 random EOC scores between 0 and 1\n",
    "    'response': np.random.choice([0, 1, 2, 3, 4, 5], 1000),  # 1000 random responses between 0 and 5\n",
    "    'n_attempt': np.random.randint(1, 300, 1000)  # 1000 random attempts between 1 and 300\n",
    "}\n",
    "\n",
    "# This is the simulated DataFrame\n",
    "d3 = pd.DataFrame(dummy_data)\n",
    "\n",
    "# Condition to remove outliers in 'response' and 'EOC' where response is high and EOC is low or vice versa.\n",
    "high_response_low_eoc = (d3['response'] > 4) & (d3['EOC'] < 0.2)\n",
    "low_response_high_eoc = (d3['response'] < 1) & (d3['EOC'] > 0.8)\n",
    "outliers_condition = ~(high_response_low_eoc | low_response_high_eoc)\n",
    "\n",
    "# Apply the condition to filter out the outliers from 'response' and 'EOC'\n",
    "d3_filtered = d3[outliers_condition]\n",
    "\n",
    "# Additional condition to disregard more than 250 attempts.\n",
    "d3_filtered = d3_filtered[d3_filtered['n_attempt'] <= 250]\n",
    "\n",
    "# Calculate the correlation again on the filtered data\n",
    "correlation_filtered = d3_filtered['EOC'].corr(d3_filtered['n_attempt'])\n",
    "correlation_2_filtered = d3_filtered['response'].corr(d3_filtered['EOC'])\n",
    "print(\"Correlation between EOC and number of attempts (filtered):\", correlation_filtered)\n",
    "print(\"Correlation between response and EOC (filtered):\", correlation_2_filtered)\n",
    "\n",
    "# Let's visualize the filtered data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(d3_filtered['response'], d3_filtered['EOC'], alpha=0.5)\n",
    "plt.title('Scatter Plot of EOC vs. Response (Outliers Removed)')\n",
    "plt.xlabel('Response (Perceived Investment of Time)')\n",
    "plt.ylabel('EOC (Success Rate)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
